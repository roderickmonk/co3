The high level goal:

The current toy problem that we are trying to solve ( as outlined in the python framework file and in our discussions ) is designed to require the learning of a model of trade size arrival (a model of what the size of the next trade or trades is likely to be). The reason to start here is that the role of trade size in our trading profits is relatively well understood and relatively easy to conceptualize. Also, we have a lot of experience with exploiting trade size arrival insights. Another reason is that the primary data required to model trade size arrival is trade history data, which for us is relatively abundant and easy to work with (as compared to order book data).

Trade size arrival is probabilistic, and the possible size of any trade spans a wide continuous range of values ( for instance, from 0.0005 BTC to 100 BTC). The simplest strategy when needing to predict something that is probabilistic is to instead predict a parameter of its probability distribution, such as the mean, median, mode, etc. However, in our case, any such aggregate would result in an excessive loss of information. Ideally, what we need is to predict the probability distribution of the size of the next trade. However, most ML/DL systems are not designed to learn to output entire probability distributions. Therefore, the approach we are working on now is to predict something that is a function of the trade size probability distribution. Specificaly, we are trying to predict the optimal order depth, which is a function of order depth payoffs, which are in turn a function of the trade size distribution. Therefore, if we can do a good job of learning the optimal order depth, then it implies that the NN has learned an internal model of the relationship between trade size distribution and preceding trade history. 



Theoretical details of the toy problem:

Imagine a betting game where you have the opportunity to bet double or nothing on the roll of a die. If the die rolls a one, you earn double your wager; if the die rolls any other number you lose your wager. If the bet size is x, then the EV (expected value) of the bet would be ( -x * (5/6)) + ( x * (1/6)) = -4x / 6 = -2x / 3. Clearly, this is not a favorable bet to take. However, what if you took the bet and won? Would it be accurate to say that you made the right decision? Or would it be accurate to say that you took the wrong decision but then got lucky? If you were trying to decide whether to take another bet with the same rules, clearly the fact that you got lucky the first time would have no bearing on what you should do next. 

This example has a direct parallel to the markets, where no outcome is certain and every participant must play the odds. In trading, sometimes you do the right thing and are punished; sometimes you do the wrong thing and are rewarded. We can understand market events as the outcome of a stochastic process (like the die in our example), albeit one that is abstract and hidden to us. Since the stochastic process that governs the market is hidden, we must infer it by observing its outcomes. In our game example, imagine if we had no knowledge of how a die works or of how payouts are determined. Then, we would have to keep betting (or not betting) until we accrued a sample of rolls large enough to infer that the EV of placing a bet is negative and that the EV of not placing a bet is zero. This is an example of an n-armed bandit (2-armed in this case; bet or don't bet).

In our example, the statistics of the game make it relatively easy to find the optimal solution, because the optimal action is to not place a bet and most trials yield a negative bet payout that confirms that fact. However, imagine a game with a 100 sided dice where the payout for rolling a 1 is 149 times the wager, and all other rolls still result in the loss of the wager. Then the EV of placing a bet would be positive: ( -x * ( 99/100 )) + ( x * (149/100)) = -99x/100  + 149x/100  = 50x / 100 = x/2. However, 99% of die rolls under these rules would reinforce the false conclusion that placing a bet has negative EV, and it would take many trials to acertain that the EV is positive. 

Now lets imagine a third game, a game that is a sort of combination of the two above games. In the third game, each die roll and payout randomly alternate between the rules of the two previous games; some die rolls are a six sided die with negative EV, and the other die rolls are a one hundred sided die with positive EV. To win at this third game, one would need to place a bet on the 100-sided rolls, and refrain from betting on the 6-sided rolls. In order to be able to enact that strategy, there would have to be some statistical basis for predicting which die type will prevail on the next roll. Let us imagine that there is some signal that could be used to predict, with sufficient accuracy, which regime will prevail on the next roll. Now, imagine that we want to use supervised learning to train a classifier to output a betting decision (bet or don't bet) based on that signal. Pursuant to that training, imagine we also have labeled data showing what would have been the highest paying bet decision for each trial. Thus, that labeled data would tell us that the correct answer was to place a bet whenever a one was rolled, and it would tell us that the correct answer was to not place a bet whenever a one was not rolled (keep in mind though that the actual die rolls are not part of the labeled data, just the optimal bet choice). This is problematic, because the labeled data would therefore be telling us that it is sometimes correct to place a bet on the 6-sided die, and almost always incorrect to place a bet on the 100-sided die, even though both of those conclusions are in error. In reality, we should never bet on the 6 sided die, and we should always bet on the 100 sided die. If we assume for the sake of argument that our training protocal would not result in any overfitting, then the result of naively training a classifier under these conditions would be a classifier that always chooses not to place a bet, because, under both conditions that can be detected from the signal, the majority of outputs call for not placing a bet. 

The third game is an analogy for the current toy problem that we are trying to solve. The 6 and 100 sided dice represent quiet and active market conditions respectively. The decision to bet or not bet represents the decision to either place an order deep in the order book or at the top of the order book respectively. The negative EV for the six sided die and the positive EV for the 100 sided die represent the dynamic that deep orders will be optimal in an active market becuase they will be filled more frequently in an active market than in a quiet market, while shallow orders will be optimal in a quiet market for the inverse reason. The signal mentioned above represents the preceding trade history, which would be the basis for the NN to detect whether the market is in an active or quiet regime. 

The fundamental issue then is that the probabilistic nature of payoffs as a function of order depth decisions makes it complicated to use those payoffs to determine a training error or the like. Also, fundamentally, the scenario appears to be an n-armed bandit, and for that reason a reinforcement learning protocol may be appropriate. 